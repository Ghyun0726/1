{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1OZBlqRq8c6qh3x/ffGJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghyun0726/1/blob/main/Week3_%EA%B3%BC%EC%A0%9C(Code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnfgc91tcLZW",
        "outputId": "c42524af-b79e-46ca-b493-d4c60b243f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "## Lecture 1\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lecture 2\n",
        "w = 1.0\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "def loss(x,y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)"
      ],
      "metadata": {
        "id": "OfyM7cqPfgRK"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_list = [ ]\n",
        "mse_list = [ ]\n",
        "for w in np.arange(0.0,4.1,0.1):\n",
        "    print(\"w=\",w)\n",
        "    l_sum = 0\n",
        "    for x_val,y_val in zip(x_data,y_data):\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val,y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\",x_val,y_val,y_pred_val,l)\n",
        "\n",
        "    print(\"MSE=\",l_sum/3)\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum/3)\n",
        "\n",
        "plt.plot(w_list,mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cm48eHEriafm",
        "outputId": "54e4c307-c3f8-4060-d1f1-307e72ea3bb9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w= 0.0\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.]) tensor([4.])\n",
            "\t tensor([2.]) tensor([4.]) tensor([0.]) tensor([16.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([0.]) tensor([36.])\n",
            "MSE= tensor([18.6667])\n",
            "w= 0.1\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.1000]) tensor([3.6100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([0.2000]) tensor([14.4400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([0.3000]) tensor([32.4900])\n",
            "MSE= tensor([16.8467])\n",
            "w= 0.2\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.2000]) tensor([3.2400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([0.4000]) tensor([12.9600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([0.6000]) tensor([29.1600])\n",
            "MSE= tensor([15.1200])\n",
            "w= 0.30000000000000004\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.3000]) tensor([2.8900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([0.6000]) tensor([11.5600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([0.9000]) tensor([26.0100])\n",
            "MSE= tensor([13.4867])\n",
            "w= 0.4\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.4000]) tensor([2.5600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([0.8000]) tensor([10.2400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([1.2000]) tensor([23.0400])\n",
            "MSE= tensor([11.9467])\n",
            "w= 0.5\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.5000]) tensor([2.2500])\n",
            "\t tensor([2.]) tensor([4.]) tensor([1.]) tensor([9.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([1.5000]) tensor([20.2500])\n",
            "MSE= tensor([10.5000])\n",
            "w= 0.6000000000000001\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.6000]) tensor([1.9600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([1.2000]) tensor([7.8400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([1.8000]) tensor([17.6400])\n",
            "MSE= tensor([9.1467])\n",
            "w= 0.7000000000000001\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.7000]) tensor([1.6900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([1.4000]) tensor([6.7600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([2.1000]) tensor([15.2100])\n",
            "MSE= tensor([7.8867])\n",
            "w= 0.8\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.8000]) tensor([1.4400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([1.6000]) tensor([5.7600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([2.4000]) tensor([12.9600])\n",
            "MSE= tensor([6.7200])\n",
            "w= 0.9\n",
            "\t tensor([1.]) tensor([2.]) tensor([0.9000]) tensor([1.2100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([1.8000]) tensor([4.8400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([2.7000]) tensor([10.8900])\n",
            "MSE= tensor([5.6467])\n",
            "w= 1.0\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.]) tensor([1.])\n",
            "\t tensor([2.]) tensor([4.]) tensor([2.]) tensor([4.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([3.]) tensor([9.])\n",
            "MSE= tensor([4.6667])\n",
            "w= 1.1\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.1000]) tensor([0.8100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([2.2000]) tensor([3.2400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([3.3000]) tensor([7.2900])\n",
            "MSE= tensor([3.7800])\n",
            "w= 1.2000000000000002\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.2000]) tensor([0.6400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([2.4000]) tensor([2.5600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([3.6000]) tensor([5.7600])\n",
            "MSE= tensor([2.9867])\n",
            "w= 1.3\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.3000]) tensor([0.4900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([2.6000]) tensor([1.9600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([3.9000]) tensor([4.4100])\n",
            "MSE= tensor([2.2867])\n",
            "w= 1.4000000000000001\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.4000]) tensor([0.3600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([2.8000]) tensor([1.4400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([4.2000]) tensor([3.2400])\n",
            "MSE= tensor([1.6800])\n",
            "w= 1.5\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.5000]) tensor([0.2500])\n",
            "\t tensor([2.]) tensor([4.]) tensor([3.]) tensor([1.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([4.5000]) tensor([2.2500])\n",
            "MSE= tensor([1.1667])\n",
            "w= 1.6\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.6000]) tensor([0.1600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([3.2000]) tensor([0.6400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([4.8000]) tensor([1.4400])\n",
            "MSE= tensor([0.7467])\n",
            "w= 1.7000000000000002\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.7000]) tensor([0.0900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([3.4000]) tensor([0.3600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([5.1000]) tensor([0.8100])\n",
            "MSE= tensor([0.4200])\n",
            "w= 1.8\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.8000]) tensor([0.0400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([3.6000]) tensor([0.1600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([5.4000]) tensor([0.3600])\n",
            "MSE= tensor([0.1867])\n",
            "w= 1.9000000000000001\n",
            "\t tensor([1.]) tensor([2.]) tensor([1.9000]) tensor([0.0100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([3.8000]) tensor([0.0400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([5.7000]) tensor([0.0900])\n",
            "MSE= tensor([0.0467])\n",
            "w= 2.0\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.]) tensor([0.])\n",
            "\t tensor([2.]) tensor([4.]) tensor([4.]) tensor([0.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([6.]) tensor([0.])\n",
            "MSE= tensor([0.])\n",
            "w= 2.1\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.1000]) tensor([0.0100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([4.2000]) tensor([0.0400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([6.3000]) tensor([0.0900])\n",
            "MSE= tensor([0.0467])\n",
            "w= 2.2\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.2000]) tensor([0.0400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([4.4000]) tensor([0.1600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([6.6000]) tensor([0.3600])\n",
            "MSE= tensor([0.1867])\n",
            "w= 2.3000000000000003\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.3000]) tensor([0.0900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([4.6000]) tensor([0.3600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([6.9000]) tensor([0.8100])\n",
            "MSE= tensor([0.4200])\n",
            "w= 2.4000000000000004\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.4000]) tensor([0.1600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([4.8000]) tensor([0.6400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([7.2000]) tensor([1.4400])\n",
            "MSE= tensor([0.7467])\n",
            "w= 2.5\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.5000]) tensor([0.2500])\n",
            "\t tensor([2.]) tensor([4.]) tensor([5.]) tensor([1.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([7.5000]) tensor([2.2500])\n",
            "MSE= tensor([1.1667])\n",
            "w= 2.6\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.6000]) tensor([0.3600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([5.2000]) tensor([1.4400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([7.8000]) tensor([3.2400])\n",
            "MSE= tensor([1.6800])\n",
            "w= 2.7\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.7000]) tensor([0.4900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([5.4000]) tensor([1.9600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([8.1000]) tensor([4.4100])\n",
            "MSE= tensor([2.2867])\n",
            "w= 2.8000000000000003\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.8000]) tensor([0.6400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([5.6000]) tensor([2.5600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([8.4000]) tensor([5.7600])\n",
            "MSE= tensor([2.9867])\n",
            "w= 2.9000000000000004\n",
            "\t tensor([1.]) tensor([2.]) tensor([2.9000]) tensor([0.8100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([5.8000]) tensor([3.2400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([8.7000]) tensor([7.2900])\n",
            "MSE= tensor([3.7800])\n",
            "w= 3.0\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.]) tensor([1.])\n",
            "\t tensor([2.]) tensor([4.]) tensor([6.]) tensor([4.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([9.]) tensor([9.])\n",
            "MSE= tensor([4.6667])\n",
            "w= 3.1\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.1000]) tensor([1.2100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([6.2000]) tensor([4.8400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([9.3000]) tensor([10.8900])\n",
            "MSE= tensor([5.6467])\n",
            "w= 3.2\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.2000]) tensor([1.4400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([6.4000]) tensor([5.7600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([9.6000]) tensor([12.9600])\n",
            "MSE= tensor([6.7200])\n",
            "w= 3.3000000000000003\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.3000]) tensor([1.6900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([6.6000]) tensor([6.7600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([9.9000]) tensor([15.2100])\n",
            "MSE= tensor([7.8867])\n",
            "w= 3.4000000000000004\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.4000]) tensor([1.9600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([6.8000]) tensor([7.8400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([10.2000]) tensor([17.6400])\n",
            "MSE= tensor([9.1467])\n",
            "w= 3.5\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.5000]) tensor([2.2500])\n",
            "\t tensor([2.]) tensor([4.]) tensor([7.]) tensor([9.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([10.5000]) tensor([20.2500])\n",
            "MSE= tensor([10.5000])\n",
            "w= 3.6\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.6000]) tensor([2.5600])\n",
            "\t tensor([2.]) tensor([4.]) tensor([7.2000]) tensor([10.2400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([10.8000]) tensor([23.0400])\n",
            "MSE= tensor([11.9467])\n",
            "w= 3.7\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.7000]) tensor([2.8900])\n",
            "\t tensor([2.]) tensor([4.]) tensor([7.4000]) tensor([11.5600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([11.1000]) tensor([26.0100])\n",
            "MSE= tensor([13.4867])\n",
            "w= 3.8000000000000003\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.8000]) tensor([3.2400])\n",
            "\t tensor([2.]) tensor([4.]) tensor([7.6000]) tensor([12.9600])\n",
            "\t tensor([3.]) tensor([6.]) tensor([11.4000]) tensor([29.1600])\n",
            "MSE= tensor([15.1200])\n",
            "w= 3.9000000000000004\n",
            "\t tensor([1.]) tensor([2.]) tensor([3.9000]) tensor([3.6100])\n",
            "\t tensor([2.]) tensor([4.]) tensor([7.8000]) tensor([14.4400])\n",
            "\t tensor([3.]) tensor([6.]) tensor([11.7000]) tensor([32.4900])\n",
            "MSE= tensor([16.8467])\n",
            "w= 4.0\n",
            "\t tensor([1.]) tensor([2.]) tensor([4.]) tensor([4.])\n",
            "\t tensor([2.]) tensor([4.]) tensor([8.]) tensor([16.])\n",
            "\t tensor([3.]) tensor([6.]) tensor([12.]) tensor([36.])\n",
            "MSE= tensor([18.6667])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lecture 3\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w=1.0\n",
        "\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "def loss(x,y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "def gradient(x,y):\n",
        "    return 2 * x * (x * w - y)\n",
        "\n",
        "print(\"predict (before training)\",4,forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "    for x_val,y_val in zip(x_data,y_data):\n",
        "        grad = gradient(x_val,y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        print(\"\\tgrad:\",x_val,y_val,grad)\n",
        "        l = loss(x_val,y_val)\n",
        "\n",
        "    print(\"progress:\",epoch,\"w=\",w,\"loss=\",l)\n",
        "\n",
        "print(\"predict (after training)\",4,forward(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhk8rv3KlNU0",
        "outputId": "4b7152ef-d01a-476d-cc1e-1891828ebecd"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad: 1.0 2.0 -2.0\n",
            "\tgrad: 2.0 4.0 -7.84\n",
            "\tgrad: 3.0 6.0 -16.2288\n",
            "progress: 0 w= 1.260688 loss= 4.919240100095999\n",
            "\tgrad: 1.0 2.0 -1.478624\n",
            "\tgrad: 2.0 4.0 -5.796206079999999\n",
            "\tgrad: 3.0 6.0 -11.998146585599997\n",
            "progress: 1 w= 1.453417766656 loss= 2.688769240265834\n",
            "\tgrad: 1.0 2.0 -1.093164466688\n",
            "\tgrad: 2.0 4.0 -4.285204709416961\n",
            "\tgrad: 3.0 6.0 -8.87037374849311\n",
            "progress: 2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
            "\tgrad: 1.0 2.0 -0.8081896081960389\n",
            "\tgrad: 2.0 4.0 -3.1681032641284723\n",
            "\tgrad: 3.0 6.0 -6.557973756745939\n",
            "progress: 3 w= 1.701247862192685 loss= 0.8032755585999681\n",
            "\tgrad: 1.0 2.0 -0.59750427561463\n",
            "\tgrad: 2.0 4.0 -2.3422167604093502\n",
            "\tgrad: 3.0 6.0 -4.848388694047353\n",
            "progress: 4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
            "\tgrad: 1.0 2.0 -0.44174208101320334\n",
            "\tgrad: 2.0 4.0 -1.7316289575717576\n",
            "\tgrad: 3.0 6.0 -3.584471942173538\n",
            "progress: 5 w= 1.836707389300983 loss= 0.2399802903801062\n",
            "\tgrad: 1.0 2.0 -0.3265852213980338\n",
            "\tgrad: 2.0 4.0 -1.2802140678802925\n",
            "\tgrad: 3.0 6.0 -2.650043120512205\n",
            "progress: 6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
            "\tgrad: 1.0 2.0 -0.241448373202223\n",
            "\tgrad: 2.0 4.0 -0.946477622952715\n",
            "\tgrad: 3.0 6.0 -1.9592086795121197\n",
            "progress: 7 w= 1.910747160155559 loss= 0.07169462478267678\n",
            "\tgrad: 1.0 2.0 -0.17850567968888198\n",
            "\tgrad: 2.0 4.0 -0.6997422643804168\n",
            "\tgrad: 3.0 6.0 -1.4484664872674653\n",
            "progress: 8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
            "\tgrad: 1.0 2.0 -0.13197139106214673\n",
            "\tgrad: 2.0 4.0 -0.5173278529636143\n",
            "\tgrad: 3.0 6.0 -1.0708686556346834\n",
            "progress: 9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
            "\tgrad: 1.0 2.0 -0.09756803306893769\n",
            "\tgrad: 2.0 4.0 -0.38246668963023644\n",
            "\tgrad: 3.0 6.0 -0.7917060475345892\n",
            "progress: 10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
            "\tgrad: 1.0 2.0 -0.07213321766426262\n",
            "\tgrad: 2.0 4.0 -0.2827622132439096\n",
            "\tgrad: 3.0 6.0 -0.5853177814148953\n",
            "progress: 11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
            "\tgrad: 1.0 2.0 -0.05332895341780164\n",
            "\tgrad: 2.0 4.0 -0.2090494973977819\n",
            "\tgrad: 3.0 6.0 -0.4327324596134101\n",
            "progress: 12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
            "\tgrad: 1.0 2.0 -0.039426735209221686\n",
            "\tgrad: 2.0 4.0 -0.15455280202014876\n",
            "\tgrad: 3.0 6.0 -0.3199243001817109\n",
            "progress: 13 w= 1.9854256707695 loss= 0.001911699652671057\n",
            "\tgrad: 1.0 2.0 -0.02914865846100012\n",
            "\tgrad: 2.0 4.0 -0.11426274116712065\n",
            "\tgrad: 3.0 6.0 -0.2365238742159388\n",
            "progress: 14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
            "\tgrad: 1.0 2.0 -0.021549952984118992\n",
            "\tgrad: 2.0 4.0 -0.08447581569774698\n",
            "\tgrad: 3.0 6.0 -0.17486493849433593\n",
            "progress: 15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
            "\tgrad: 1.0 2.0 -0.015932138840594856\n",
            "\tgrad: 2.0 4.0 -0.062453984255132156\n",
            "\tgrad: 3.0 6.0 -0.12927974740812687\n",
            "progress: 16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
            "\tgrad: 1.0 2.0 -0.011778821430517894\n",
            "\tgrad: 2.0 4.0 -0.046172980007630926\n",
            "\tgrad: 3.0 6.0 -0.09557806861579543\n",
            "progress: 17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
            "\tgrad: 1.0 2.0 -0.008708224029438938\n",
            "\tgrad: 2.0 4.0 -0.03413623819540135\n",
            "\tgrad: 3.0 6.0 -0.07066201306448505\n",
            "progress: 18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
            "\tgrad: 1.0 2.0 -0.006438094523652627\n",
            "\tgrad: 2.0 4.0 -0.02523733053271826\n",
            "\tgrad: 3.0 6.0 -0.052241274202728505\n",
            "progress: 19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
            "\tgrad: 1.0 2.0 -0.004759760538470381\n",
            "\tgrad: 2.0 4.0 -0.01865826131080439\n",
            "\tgrad: 3.0 6.0 -0.03862260091336722\n",
            "progress: 20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
            "\tgrad: 1.0 2.0 -0.0035189480832178432\n",
            "\tgrad: 2.0 4.0 -0.01379427648621423\n",
            "\tgrad: 3.0 6.0 -0.028554152326460525\n",
            "progress: 21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
            "\tgrad: 1.0 2.0 -0.002601600545300009\n",
            "\tgrad: 2.0 4.0 -0.01019827413757568\n",
            "\tgrad: 3.0 6.0 -0.021110427464781978\n",
            "progress: 22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
            "\tgrad: 1.0 2.0 -0.001923394502346909\n",
            "\tgrad: 2.0 4.0 -0.007539706449199102\n",
            "\tgrad: 3.0 6.0 -0.01560719234984198\n",
            "progress: 23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
            "\tgrad: 1.0 2.0 -0.0014219886363191492\n",
            "\tgrad: 2.0 4.0 -0.005574195454370212\n",
            "\tgrad: 3.0 6.0 -0.011538584590544687\n",
            "progress: 24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
            "\tgrad: 1.0 2.0 -0.0010512932626940419\n",
            "\tgrad: 2.0 4.0 -0.004121069589761106\n",
            "\tgrad: 3.0 6.0 -0.008530614050808794\n",
            "progress: 25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
            "\tgrad: 1.0 2.0 -0.0007772337246287897\n",
            "\tgrad: 2.0 4.0 -0.0030467562005451754\n",
            "\tgrad: 3.0 6.0 -0.006306785335127074\n",
            "progress: 26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
            "\tgrad: 1.0 2.0 -0.0005746182194226179\n",
            "\tgrad: 2.0 4.0 -0.002252503420136165\n",
            "\tgrad: 3.0 6.0 -0.00466268207967957\n",
            "progress: 27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
            "\tgrad: 1.0 2.0 -0.0004248221450375844\n",
            "\tgrad: 2.0 4.0 -0.0016653028085471533\n",
            "\tgrad: 3.0 6.0 -0.0034471768136938863\n",
            "progress: 28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
            "\tgrad: 1.0 2.0 -0.00031407610969225175\n",
            "\tgrad: 2.0 4.0 -0.0012311783499932005\n",
            "\tgrad: 3.0 6.0 -0.0025485391844828342\n",
            "progress: 29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
            "\tgrad: 1.0 2.0 -0.00023220023680847746\n",
            "\tgrad: 2.0 4.0 -0.0009102249282886277\n",
            "\tgrad: 3.0 6.0 -0.0018841656015560204\n",
            "progress: 30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
            "\tgrad: 1.0 2.0 -0.00017166842147497974\n",
            "\tgrad: 2.0 4.0 -0.0006729402121816719\n",
            "\tgrad: 3.0 6.0 -0.0013929862392156878\n",
            "progress: 31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
            "\tgrad: 1.0 2.0 -0.0001269165240174175\n",
            "\tgrad: 2.0 4.0 -0.0004975127741477792\n",
            "\tgrad: 3.0 6.0 -0.0010298514424817995\n",
            "progress: 32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
            "\tgrad: 1.0 2.0 -9.383090920422887e-05\n",
            "\tgrad: 2.0 4.0 -0.00036781716408107457\n",
            "\tgrad: 3.0 6.0 -0.0007613815296476645\n",
            "progress: 33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
            "\tgrad: 1.0 2.0 -6.937031714571162e-05\n",
            "\tgrad: 2.0 4.0 -0.0002719316432120422\n",
            "\tgrad: 3.0 6.0 -0.0005628985014531906\n",
            "progress: 34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
            "\tgrad: 1.0 2.0 -5.1286307909848006e-05\n",
            "\tgrad: 2.0 4.0 -0.00020104232700646207\n",
            "\tgrad: 3.0 6.0 -0.0004161576169003922\n",
            "progress: 35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
            "\tgrad: 1.0 2.0 -3.7916582873442906e-05\n",
            "\tgrad: 2.0 4.0 -0.0001486330048638962\n",
            "\tgrad: 3.0 6.0 -0.0003076703200690645\n",
            "progress: 36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
            "\tgrad: 1.0 2.0 -2.8032184717474706e-05\n",
            "\tgrad: 2.0 4.0 -0.0001098861640933535\n",
            "\tgrad: 3.0 6.0 -0.00022746435967313516\n",
            "progress: 37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
            "\tgrad: 1.0 2.0 -2.0724530547688857e-05\n",
            "\tgrad: 2.0 4.0 -8.124015974608767e-05\n",
            "\tgrad: 3.0 6.0 -0.00016816713067413502\n",
            "progress: 38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
            "\tgrad: 1.0 2.0 -1.5321894128117464e-05\n",
            "\tgrad: 2.0 4.0 -6.006182498197177e-05\n",
            "\tgrad: 3.0 6.0 -0.00012432797771566584\n",
            "progress: 39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
            "\tgrad: 1.0 2.0 -1.1327660191629008e-05\n",
            "\tgrad: 2.0 4.0 -4.4404427951505454e-05\n",
            "\tgrad: 3.0 6.0 -9.191716585732479e-05\n",
            "progress: 40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
            "\tgrad: 1.0 2.0 -8.37467511161094e-06\n",
            "\tgrad: 2.0 4.0 -3.282872643772805e-05\n",
            "\tgrad: 3.0 6.0 -6.795546372551087e-05\n",
            "progress: 41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
            "\tgrad: 1.0 2.0 -6.191497806007362e-06\n",
            "\tgrad: 2.0 4.0 -2.4270671399762023e-05\n",
            "\tgrad: 3.0 6.0 -5.0240289795056015e-05\n",
            "progress: 42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
            "\tgrad: 1.0 2.0 -4.5774486259198e-06\n",
            "\tgrad: 2.0 4.0 -1.794359861406747e-05\n",
            "\tgrad: 3.0 6.0 -3.714324913239864e-05\n",
            "progress: 43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
            "\tgrad: 1.0 2.0 -3.3841626985164908e-06\n",
            "\tgrad: 2.0 4.0 -1.326591777761621e-05\n",
            "\tgrad: 3.0 6.0 -2.7460449796734565e-05\n",
            "progress: 44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
            "\tgrad: 1.0 2.0 -2.5019520926150562e-06\n",
            "\tgrad: 2.0 4.0 -9.807652203264183e-06\n",
            "\tgrad: 3.0 6.0 -2.0301840059744336e-05\n",
            "progress: 45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
            "\tgrad: 1.0 2.0 -1.8497232057157476e-06\n",
            "\tgrad: 2.0 4.0 -7.250914967116273e-06\n",
            "\tgrad: 3.0 6.0 -1.5009393983689279e-05\n",
            "progress: 46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
            "\tgrad: 1.0 2.0 -1.3675225627451937e-06\n",
            "\tgrad: 2.0 4.0 -5.3606884460322135e-06\n",
            "\tgrad: 3.0 6.0 -1.109662508014253e-05\n",
            "progress: 47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
            "\tgrad: 1.0 2.0 -1.0110258408246864e-06\n",
            "\tgrad: 2.0 4.0 -3.963221296032771e-06\n",
            "\tgrad: 3.0 6.0 -8.20386808086937e-06\n",
            "progress: 48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
            "\tgrad: 1.0 2.0 -7.474635363990956e-07\n",
            "\tgrad: 2.0 4.0 -2.930057062755509e-06\n",
            "\tgrad: 3.0 6.0 -6.065218119744031e-06\n",
            "progress: 49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
            "\tgrad: 1.0 2.0 -5.526087618612507e-07\n",
            "\tgrad: 2.0 4.0 -2.166226346744793e-06\n",
            "\tgrad: 3.0 6.0 -4.484088535150477e-06\n",
            "progress: 50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
            "\tgrad: 1.0 2.0 -4.08550288710785e-07\n",
            "\tgrad: 2.0 4.0 -1.6015171322436572e-06\n",
            "\tgrad: 3.0 6.0 -3.3151404608133817e-06\n",
            "progress: 51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
            "\tgrad: 1.0 2.0 -3.020461312175371e-07\n",
            "\tgrad: 2.0 4.0 -1.1840208351543424e-06\n",
            "\tgrad: 3.0 6.0 -2.4509231284497446e-06\n",
            "progress: 52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
            "\tgrad: 1.0 2.0 -2.2330632942768602e-07\n",
            "\tgrad: 2.0 4.0 -8.753608113920563e-07\n",
            "\tgrad: 3.0 6.0 -1.811996877876254e-06\n",
            "progress: 53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
            "\tgrad: 1.0 2.0 -1.6509304900935717e-07\n",
            "\tgrad: 2.0 4.0 -6.471647520100987e-07\n",
            "\tgrad: 3.0 6.0 -1.3396310407642886e-06\n",
            "progress: 54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
            "\tgrad: 1.0 2.0 -1.220552721115098e-07\n",
            "\tgrad: 2.0 4.0 -4.784566662863199e-07\n",
            "\tgrad: 3.0 6.0 -9.904052991061008e-07\n",
            "progress: 55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
            "\tgrad: 1.0 2.0 -9.023692726373156e-08\n",
            "\tgrad: 2.0 4.0 -3.5372875473171916e-07\n",
            "\tgrad: 3.0 6.0 -7.322185204827747e-07\n",
            "progress: 56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
            "\tgrad: 1.0 2.0 -6.671324292994996e-08\n",
            "\tgrad: 2.0 4.0 -2.615159129248923e-07\n",
            "\tgrad: 3.0 6.0 -5.413379398078177e-07\n",
            "progress: 57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
            "\tgrad: 1.0 2.0 -4.932190122985958e-08\n",
            "\tgrad: 2.0 4.0 -1.9334185274999527e-07\n",
            "\tgrad: 3.0 6.0 -4.002176350326181e-07\n",
            "progress: 58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
            "\tgrad: 1.0 2.0 -3.6464273378555845e-08\n",
            "\tgrad: 2.0 4.0 -1.429399514307761e-07\n",
            "\tgrad: 3.0 6.0 -2.9588569994132286e-07\n",
            "progress: 59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
            "\tgrad: 1.0 2.0 -2.6958475007887728e-08\n",
            "\tgrad: 2.0 4.0 -1.0567722164012139e-07\n",
            "\tgrad: 3.0 6.0 -2.1875184863517916e-07\n",
            "progress: 60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
            "\tgrad: 1.0 2.0 -1.993072418216002e-08\n",
            "\tgrad: 2.0 4.0 -7.812843882959442e-08\n",
            "\tgrad: 3.0 6.0 -1.617258700292723e-07\n",
            "progress: 61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
            "\tgrad: 1.0 2.0 -1.473502342363986e-08\n",
            "\tgrad: 2.0 4.0 -5.7761292637792394e-08\n",
            "\tgrad: 3.0 6.0 -1.195658771990793e-07\n",
            "progress: 62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
            "\tgrad: 1.0 2.0 -1.0893780100218464e-08\n",
            "\tgrad: 2.0 4.0 -4.270361841918202e-08\n",
            "\tgrad: 3.0 6.0 -8.839649012770678e-08\n",
            "progress: 63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
            "\tgrad: 1.0 2.0 -8.05390243385773e-09\n",
            "\tgrad: 2.0 4.0 -3.1571296688071016e-08\n",
            "\tgrad: 3.0 6.0 -6.53525820126788e-08\n",
            "progress: 64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
            "\tgrad: 1.0 2.0 -5.9543463493128e-09\n",
            "\tgrad: 2.0 4.0 -2.334103754719763e-08\n",
            "\tgrad: 3.0 6.0 -4.8315948575350376e-08\n",
            "progress: 65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
            "\tgrad: 1.0 2.0 -4.402119557767037e-09\n",
            "\tgrad: 2.0 4.0 -1.725630838222969e-08\n",
            "\tgrad: 3.0 6.0 -3.5720557178819945e-08\n",
            "progress: 66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
            "\tgrad: 1.0 2.0 -3.254539748809293e-09\n",
            "\tgrad: 2.0 4.0 -1.2757796596929438e-08\n",
            "\tgrad: 3.0 6.0 -2.6408640607655798e-08\n",
            "progress: 67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
            "\tgrad: 1.0 2.0 -2.406120636067044e-09\n",
            "\tgrad: 2.0 4.0 -9.431992964437086e-09\n",
            "\tgrad: 3.0 6.0 -1.9524227568012975e-08\n",
            "progress: 68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
            "\tgrad: 1.0 2.0 -1.7788739370416806e-09\n",
            "\tgrad: 2.0 4.0 -6.97318647269185e-09\n",
            "\tgrad: 3.0 6.0 -1.4434496264925656e-08\n",
            "progress: 69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
            "\tgrad: 1.0 2.0 -1.3151431055291596e-09\n",
            "\tgrad: 2.0 4.0 -5.155360582875801e-09\n",
            "\tgrad: 3.0 6.0 -1.067159693945996e-08\n",
            "progress: 70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
            "\tgrad: 1.0 2.0 -9.72300906454393e-10\n",
            "\tgrad: 2.0 4.0 -3.811418736177075e-09\n",
            "\tgrad: 3.0 6.0 -7.88963561149103e-09\n",
            "progress: 71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
            "\tgrad: 1.0 2.0 -7.18833437218791e-10\n",
            "\tgrad: 2.0 4.0 -2.8178277489132597e-09\n",
            "\tgrad: 3.0 6.0 -5.832902161273523e-09\n",
            "progress: 72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
            "\tgrad: 1.0 2.0 -5.314420015167798e-10\n",
            "\tgrad: 2.0 4.0 -2.0832526814729135e-09\n",
            "\tgrad: 3.0 6.0 -4.31233715403323e-09\n",
            "progress: 73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
            "\tgrad: 1.0 2.0 -3.92901711165905e-10\n",
            "\tgrad: 2.0 4.0 -1.5401742103904326e-09\n",
            "\tgrad: 3.0 6.0 -3.188159070077745e-09\n",
            "progress: 74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
            "\tgrad: 1.0 2.0 -2.9047697580608656e-10\n",
            "\tgrad: 2.0 4.0 -1.1386696030513122e-09\n",
            "\tgrad: 3.0 6.0 -2.3570478902001923e-09\n",
            "progress: 75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
            "\tgrad: 1.0 2.0 -2.1475310418850313e-10\n",
            "\tgrad: 2.0 4.0 -8.418314934033333e-10\n",
            "\tgrad: 3.0 6.0 -1.7425900722400911e-09\n",
            "progress: 76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
            "\tgrad: 1.0 2.0 -1.5876944203796484e-10\n",
            "\tgrad: 2.0 4.0 -6.223768167501476e-10\n",
            "\tgrad: 3.0 6.0 -1.2883241140571045e-09\n",
            "progress: 77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
            "\tgrad: 1.0 2.0 -1.17380327679939e-10\n",
            "\tgrad: 2.0 4.0 -4.601314884666863e-10\n",
            "\tgrad: 3.0 6.0 -9.524754318590567e-10\n",
            "progress: 78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
            "\tgrad: 1.0 2.0 -8.678080476443029e-11\n",
            "\tgrad: 2.0 4.0 -3.4018121652934497e-10\n",
            "\tgrad: 3.0 6.0 -7.041780492045291e-10\n",
            "progress: 79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
            "\tgrad: 1.0 2.0 -6.415845632545825e-11\n",
            "\tgrad: 2.0 4.0 -2.5150193039280566e-10\n",
            "\tgrad: 3.0 6.0 -5.206075570640678e-10\n",
            "progress: 80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
            "\tgrad: 1.0 2.0 -4.743316850408519e-11\n",
            "\tgrad: 2.0 4.0 -1.8593837580738182e-10\n",
            "\tgrad: 3.0 6.0 -3.8489211817704927e-10\n",
            "progress: 81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
            "\tgrad: 1.0 2.0 -3.5067948545020045e-11\n",
            "\tgrad: 2.0 4.0 -1.3746692673066718e-10\n",
            "\tgrad: 3.0 6.0 -2.845563784603655e-10\n",
            "progress: 82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
            "\tgrad: 1.0 2.0 -2.5926372160256506e-11\n",
            "\tgrad: 2.0 4.0 -1.0163070385260653e-10\n",
            "\tgrad: 3.0 6.0 -2.1037571684701106e-10\n",
            "progress: 83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
            "\tgrad: 1.0 2.0 -1.9167778475548403e-11\n",
            "\tgrad: 2.0 4.0 -7.51381179497912e-11\n",
            "\tgrad: 3.0 6.0 -1.5553425214420713e-10\n",
            "progress: 84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
            "\tgrad: 1.0 2.0 -1.4170886686315498e-11\n",
            "\tgrad: 2.0 4.0 -5.555023108172463e-11\n",
            "\tgrad: 3.0 6.0 -1.1499068364173581e-10\n",
            "progress: 85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
            "\tgrad: 1.0 2.0 -1.0476508549572827e-11\n",
            "\tgrad: 2.0 4.0 -4.106759377009439e-11\n",
            "\tgrad: 3.0 6.0 -8.500933290633839e-11\n",
            "progress: 86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
            "\tgrad: 1.0 2.0 -7.745359908994942e-12\n",
            "\tgrad: 2.0 4.0 -3.036149109902908e-11\n",
            "\tgrad: 3.0 6.0 -6.285105769165966e-11\n",
            "progress: 87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
            "\tgrad: 1.0 2.0 -5.726086271806707e-12\n",
            "\tgrad: 2.0 4.0 -2.2446045022661565e-11\n",
            "\tgrad: 3.0 6.0 -4.646416584819235e-11\n",
            "progress: 88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
            "\tgrad: 1.0 2.0 -4.233058348290797e-12\n",
            "\tgrad: 2.0 4.0 -1.659294923683774e-11\n",
            "\tgrad: 3.0 6.0 -3.4351188560322043e-11\n",
            "progress: 89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
            "\tgrad: 1.0 2.0 -3.1294966618133913e-12\n",
            "\tgrad: 2.0 4.0 -1.226752033289813e-11\n",
            "\tgrad: 3.0 6.0 -2.539835008974478e-11\n",
            "progress: 90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
            "\tgrad: 1.0 2.0 -2.3137047833188262e-12\n",
            "\tgrad: 2.0 4.0 -9.070078021977679e-12\n",
            "\tgrad: 3.0 6.0 -1.8779644506139448e-11\n",
            "progress: 91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
            "\tgrad: 1.0 2.0 -1.7106316363424412e-12\n",
            "\tgrad: 2.0 4.0 -6.7057470687359455e-12\n",
            "\tgrad: 3.0 6.0 -1.3882228699912957e-11\n",
            "progress: 92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
            "\tgrad: 1.0 2.0 -1.2647660696529783e-12\n",
            "\tgrad: 2.0 4.0 -4.957811938766099e-12\n",
            "\tgrad: 3.0 6.0 -1.0263789818054647e-11\n",
            "progress: 93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
            "\tgrad: 1.0 2.0 -9.352518759442319e-13\n",
            "\tgrad: 2.0 4.0 -3.666400516522117e-12\n",
            "\tgrad: 3.0 6.0 -7.58859641791787e-12\n",
            "progress: 94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
            "\tgrad: 1.0 2.0 -6.914468997365475e-13\n",
            "\tgrad: 2.0 4.0 -2.7107205369247822e-12\n",
            "\tgrad: 3.0 6.0 -5.611511255665391e-12\n",
            "progress: 95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
            "\tgrad: 1.0 2.0 -5.111466805374221e-13\n",
            "\tgrad: 2.0 4.0 -2.0037305148434825e-12\n",
            "\tgrad: 3.0 6.0 -4.1460168631601846e-12\n",
            "progress: 96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
            "\tgrad: 1.0 2.0 -3.779199175824033e-13\n",
            "\tgrad: 2.0 4.0 -1.4814816040598089e-12\n",
            "\tgrad: 3.0 6.0 -3.064215547965432e-12\n",
            "progress: 97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
            "\tgrad: 1.0 2.0 -2.793321129956894e-13\n",
            "\tgrad: 2.0 4.0 -1.0942358130705543e-12\n",
            "\tgrad: 3.0 6.0 -2.2648549702353193e-12\n",
            "progress: 98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
            "\tgrad: 1.0 2.0 -2.0650148258027912e-13\n",
            "\tgrad: 2.0 4.0 -8.100187187665142e-13\n",
            "\tgrad: 3.0 6.0 -1.6786572132332367e-12\n",
            "progress: 99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
            "predict (after training) 4 7.9999999999996945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Lecture 4\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w=Variable(torch.Tensor([1.0]),requires_grad=True)\n",
        "def forward(x):\n",
        "    return x * w\n",
        "def loss(x,y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "print(\"predict (before training)\",4,forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "    for x_val,y_val in zip(x_data,y_data):\n",
        "        l = loss(x_val,y_val)\n",
        "        l.backward()\n",
        "        print(\"\\tgrad:\",x_val,y_val,w.grad.data[0])\n",
        "        w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "        w.grad.data.zero_()\n",
        "    print(\"progress:\", epoch, l.data[0])\n",
        "\n",
        "print(\"predict (after training)\",4,forward(4).data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvUyNRSFoVk2",
        "outputId": "e5ed26cc-1472-4d35-d0fb-09ec66c2e014"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict (before training) 4 tensor([4.], grad_fn=<MulBackward0>)\n",
            "\tgrad: 1.0 2.0 tensor(-2.)\n",
            "\tgrad: 2.0 4.0 tensor(-7.8400)\n",
            "\tgrad: 3.0 6.0 tensor(-16.2288)\n",
            "progress: 0 tensor(7.3159)\n",
            "\tgrad: 1.0 2.0 tensor(-1.4786)\n",
            "\tgrad: 2.0 4.0 tensor(-5.7962)\n",
            "\tgrad: 3.0 6.0 tensor(-11.9981)\n",
            "progress: 1 tensor(3.9988)\n",
            "\tgrad: 1.0 2.0 tensor(-1.0932)\n",
            "\tgrad: 2.0 4.0 tensor(-4.2852)\n",
            "\tgrad: 3.0 6.0 tensor(-8.8704)\n",
            "progress: 2 tensor(2.1857)\n",
            "\tgrad: 1.0 2.0 tensor(-0.8082)\n",
            "\tgrad: 2.0 4.0 tensor(-3.1681)\n",
            "\tgrad: 3.0 6.0 tensor(-6.5580)\n",
            "progress: 3 tensor(1.1946)\n",
            "\tgrad: 1.0 2.0 tensor(-0.5975)\n",
            "\tgrad: 2.0 4.0 tensor(-2.3422)\n",
            "\tgrad: 3.0 6.0 tensor(-4.8484)\n",
            "progress: 4 tensor(0.6530)\n",
            "\tgrad: 1.0 2.0 tensor(-0.4417)\n",
            "\tgrad: 2.0 4.0 tensor(-1.7316)\n",
            "\tgrad: 3.0 6.0 tensor(-3.5845)\n",
            "progress: 5 tensor(0.3569)\n",
            "\tgrad: 1.0 2.0 tensor(-0.3266)\n",
            "\tgrad: 2.0 4.0 tensor(-1.2802)\n",
            "\tgrad: 3.0 6.0 tensor(-2.6500)\n",
            "progress: 6 tensor(0.1951)\n",
            "\tgrad: 1.0 2.0 tensor(-0.2414)\n",
            "\tgrad: 2.0 4.0 tensor(-0.9465)\n",
            "\tgrad: 3.0 6.0 tensor(-1.9592)\n",
            "progress: 7 tensor(0.1066)\n",
            "\tgrad: 1.0 2.0 tensor(-0.1785)\n",
            "\tgrad: 2.0 4.0 tensor(-0.6997)\n",
            "\tgrad: 3.0 6.0 tensor(-1.4485)\n",
            "progress: 8 tensor(0.0583)\n",
            "\tgrad: 1.0 2.0 tensor(-0.1320)\n",
            "\tgrad: 2.0 4.0 tensor(-0.5173)\n",
            "\tgrad: 3.0 6.0 tensor(-1.0709)\n",
            "progress: 9 tensor(0.0319)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0976)\n",
            "\tgrad: 2.0 4.0 tensor(-0.3825)\n",
            "\tgrad: 3.0 6.0 tensor(-0.7917)\n",
            "progress: 10 tensor(0.0174)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0721)\n",
            "\tgrad: 2.0 4.0 tensor(-0.2828)\n",
            "\tgrad: 3.0 6.0 tensor(-0.5853)\n",
            "progress: 11 tensor(0.0095)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0533)\n",
            "\tgrad: 2.0 4.0 tensor(-0.2090)\n",
            "\tgrad: 3.0 6.0 tensor(-0.4327)\n",
            "progress: 12 tensor(0.0052)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0394)\n",
            "\tgrad: 2.0 4.0 tensor(-0.1546)\n",
            "\tgrad: 3.0 6.0 tensor(-0.3199)\n",
            "progress: 13 tensor(0.0028)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0291)\n",
            "\tgrad: 2.0 4.0 tensor(-0.1143)\n",
            "\tgrad: 3.0 6.0 tensor(-0.2365)\n",
            "progress: 14 tensor(0.0016)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0215)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0845)\n",
            "\tgrad: 3.0 6.0 tensor(-0.1749)\n",
            "progress: 15 tensor(0.0008)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0159)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0625)\n",
            "\tgrad: 3.0 6.0 tensor(-0.1293)\n",
            "progress: 16 tensor(0.0005)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0118)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0462)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0956)\n",
            "progress: 17 tensor(0.0003)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0087)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0341)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0707)\n",
            "progress: 18 tensor(0.0001)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0064)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0252)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0522)\n",
            "progress: 19 tensor(7.5804e-05)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0048)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0187)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0386)\n",
            "progress: 20 tensor(4.1433e-05)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0035)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0138)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0286)\n",
            "progress: 21 tensor(2.2647e-05)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0026)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0102)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0211)\n",
            "progress: 22 tensor(1.2377e-05)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0019)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0075)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0156)\n",
            "progress: 23 tensor(6.7684e-06)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0014)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0056)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0115)\n",
            "progress: 24 tensor(3.7001e-06)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0011)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0041)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0085)\n",
            "progress: 25 tensor(2.0219e-06)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0008)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0030)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0063)\n",
            "progress: 26 tensor(1.1045e-06)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0006)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0023)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0047)\n",
            "progress: 27 tensor(6.0411e-07)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0004)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0017)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0034)\n",
            "progress: 28 tensor(3.2960e-07)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0003)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0012)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0025)\n",
            "progress: 29 tensor(1.8051e-07)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0002)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0009)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0019)\n",
            "progress: 30 tensor(9.8744e-08)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0002)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0007)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0014)\n",
            "progress: 31 tensor(5.4148e-08)\n",
            "\tgrad: 1.0 2.0 tensor(-0.0001)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0005)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0010)\n",
            "progress: 32 tensor(2.9468e-08)\n",
            "\tgrad: 1.0 2.0 tensor(-9.3937e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0004)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0008)\n",
            "progress: 33 tensor(1.6088e-08)\n",
            "\tgrad: 1.0 2.0 tensor(-6.9380e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0003)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0006)\n",
            "progress: 34 tensor(8.7348e-09)\n",
            "\tgrad: 1.0 2.0 tensor(-5.1260e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0002)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0004)\n",
            "progress: 35 tensor(4.8467e-09)\n",
            "\tgrad: 1.0 2.0 tensor(-3.7909e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0001)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0003)\n",
            "progress: 36 tensor(2.6521e-09)\n",
            "\tgrad: 1.0 2.0 tensor(-2.8133e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-0.0001)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0002)\n",
            "progress: 37 tensor(1.4552e-09)\n",
            "\tgrad: 1.0 2.0 tensor(-2.0981e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-8.2016e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0002)\n",
            "progress: 38 tensor(7.9149e-10)\n",
            "\tgrad: 1.0 2.0 tensor(-1.5497e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-6.1035e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-0.0001)\n",
            "progress: 39 tensor(4.4020e-10)\n",
            "\tgrad: 1.0 2.0 tensor(-1.1444e-05)\n",
            "\tgrad: 2.0 4.0 tensor(-4.4823e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-9.1553e-05)\n",
            "progress: 40 tensor(2.3283e-10)\n",
            "\tgrad: 1.0 2.0 tensor(-8.3447e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-3.2425e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-6.5804e-05)\n",
            "progress: 41 tensor(1.2028e-10)\n",
            "\tgrad: 1.0 2.0 tensor(-5.9605e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-2.2888e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-4.5776e-05)\n",
            "progress: 42 tensor(5.8208e-11)\n",
            "\tgrad: 1.0 2.0 tensor(-4.2915e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-1.7166e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-3.7193e-05)\n",
            "progress: 43 tensor(3.8426e-11)\n",
            "\tgrad: 1.0 2.0 tensor(-3.3379e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-1.3351e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-2.8610e-05)\n",
            "progress: 44 tensor(2.2737e-11)\n",
            "\tgrad: 1.0 2.0 tensor(-2.6226e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-1.0490e-05)\n",
            "\tgrad: 3.0 6.0 tensor(-2.2888e-05)\n",
            "progress: 45 tensor(1.4552e-11)\n",
            "\tgrad: 1.0 2.0 tensor(-1.9073e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-7.6294e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-1.4305e-05)\n",
            "progress: 46 tensor(5.6843e-12)\n",
            "\tgrad: 1.0 2.0 tensor(-1.4305e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-5.7220e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-1.1444e-05)\n",
            "progress: 47 tensor(3.6380e-12)\n",
            "\tgrad: 1.0 2.0 tensor(-1.1921e-06)\n",
            "\tgrad: 2.0 4.0 tensor(-4.7684e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-1.1444e-05)\n",
            "progress: 48 tensor(3.6380e-12)\n",
            "\tgrad: 1.0 2.0 tensor(-9.5367e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-3.8147e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-8.5831e-06)\n",
            "progress: 49 tensor(2.0464e-12)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 50 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 51 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 52 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 53 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 54 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 55 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 56 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 57 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 58 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 59 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 60 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 61 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 62 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 63 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 64 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 65 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 66 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 67 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 68 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 69 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 70 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 71 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 72 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 73 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 74 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 75 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 76 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 77 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 78 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 79 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 80 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 81 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 82 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 83 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 84 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 85 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 86 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 87 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 88 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 89 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 90 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 91 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 92 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 93 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 94 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 95 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 96 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 97 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 98 tensor(9.0949e-13)\n",
            "\tgrad: 1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad: 2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad: 3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 99 tensor(9.0949e-13)\n",
            "predict (after training) 4 tensor(8.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lecture 5\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
        "y_data = Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model,self).__init__()\n",
        "        self.linear = torch.nn.Linear(1,1)\n",
        "    def forward(self,x):\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.MSELoss(size_average=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    loss = criterion(y_pred,y_data)\n",
        "    print(epoch, loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "print(\"predict (after training)\",4,model(hour_var).data[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVrtK5IOp9ht",
        "outputId": "0f01f553-6da7-449a-9aae-ab66193b093b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 44.822296142578125\n",
            "1 19.95584487915039\n",
            "2 8.885971069335938\n",
            "3 3.957949638366699\n",
            "4 1.764101266860962\n",
            "5 0.7874321937561035\n",
            "6 0.3526168167591095\n",
            "7 0.15901939570903778\n",
            "8 0.07280600070953369\n",
            "9 0.03439724072813988\n",
            "10 0.017270104959607124\n",
            "11 0.009617629460990429\n",
            "12 0.006183152087032795\n",
            "13 0.004626931622624397\n",
            "14 0.0039072176441550255\n",
            "15 0.0035602515563368797\n",
            "16 0.0033796329516917467\n",
            "17 0.003273413283750415\n",
            "18 0.003200726117938757\n",
            "19 0.003143305890262127\n",
            "20 0.003093043575063348\n",
            "21 0.00304632680490613\n",
            "22 0.003001546021550894\n",
            "23 0.002957946388050914\n",
            "24 0.0029152531642466784\n",
            "25 0.0028732495848089457\n",
            "26 0.002831940073519945\n",
            "27 0.0027912163641303778\n",
            "28 0.002751066815108061\n",
            "29 0.0027115403208881617\n",
            "30 0.002672589151188731\n",
            "31 0.002634152304381132\n",
            "32 0.0025962996296584606\n",
            "33 0.002559005282819271\n",
            "34 0.0025222143158316612\n",
            "35 0.002485952340066433\n",
            "36 0.0024502333253622055\n",
            "37 0.002415040275081992\n",
            "38 0.002380331512540579\n",
            "39 0.0023461133241653442\n",
            "40 0.002312377793714404\n",
            "41 0.0022791502997279167\n",
            "42 0.0022464017383754253\n",
            "43 0.0022141102235764265\n",
            "44 0.0021822848357260227\n",
            "45 0.002150935120880604\n",
            "46 0.002120002405717969\n",
            "47 0.002089566085487604\n",
            "48 0.0020595358218997717\n",
            "49 0.0020299379248172045\n",
            "50 0.002000749111175537\n",
            "51 0.0019719991832971573\n",
            "52 0.0019436543807387352\n",
            "53 0.001915738801471889\n",
            "54 0.0018881981959566474\n",
            "55 0.001861057709902525\n",
            "56 0.0018343328265473247\n",
            "57 0.001807947177439928\n",
            "58 0.0017819637432694435\n",
            "59 0.0017563472501933575\n",
            "60 0.0017311293631792068\n",
            "61 0.0017062461702153087\n",
            "62 0.00168171850964427\n",
            "63 0.0016575376503169537\n",
            "64 0.001633726991713047\n",
            "65 0.0016102436929941177\n",
            "66 0.0015871040523052216\n",
            "67 0.0015643032966181636\n",
            "68 0.0015418010298162699\n",
            "69 0.001519660116173327\n",
            "70 0.0014978200197219849\n",
            "71 0.0014763000654056668\n",
            "72 0.0014550798805430532\n",
            "73 0.0014341564383357763\n",
            "74 0.0014135473174974322\n",
            "75 0.0013932458823546767\n",
            "76 0.0013731990475207567\n",
            "77 0.0013534578029066324\n",
            "78 0.0013340369332581758\n",
            "79 0.0013148533180356026\n",
            "80 0.0012959698215126991\n",
            "81 0.0012773278867825866\n",
            "82 0.0012589849065989256\n",
            "83 0.0012408904731273651\n",
            "84 0.0012230540160089731\n",
            "85 0.0012054648250341415\n",
            "86 0.0011881391983479261\n",
            "87 0.001171076437458396\n",
            "88 0.001154245575889945\n",
            "89 0.0011376700131222606\n",
            "90 0.0011213301913812757\n",
            "91 0.0011051964247599244\n",
            "92 0.0010893053840845823\n",
            "93 0.0010736683616414666\n",
            "94 0.001058222958818078\n",
            "95 0.0010430134134367108\n",
            "96 0.0010280369315296412\n",
            "97 0.0010132456663995981\n",
            "98 0.0009987012017518282\n",
            "99 0.0009843283332884312\n",
            "100 0.0009701965609565377\n",
            "101 0.000956251285970211\n",
            "102 0.0009425150929018855\n",
            "103 0.0009289668523706496\n",
            "104 0.0009156064479611814\n",
            "105 0.0009024527971632779\n",
            "106 0.0008894792990759015\n",
            "107 0.0008766970713622868\n",
            "108 0.000864097906742245\n",
            "109 0.0008516679517924786\n",
            "110 0.0008394274627789855\n",
            "111 0.000827367533929646\n",
            "112 0.000815478793811053\n",
            "113 0.0008037732332013547\n",
            "114 0.0007922080112621188\n",
            "115 0.0007808168302290142\n",
            "116 0.0007695959066040814\n",
            "117 0.0007585483836010098\n",
            "118 0.0007476386963389814\n",
            "119 0.0007368933293037117\n",
            "120 0.000726307975128293\n",
            "121 0.000715871574357152\n",
            "122 0.0007055707974359393\n",
            "123 0.000695446040481329\n",
            "124 0.0006854446255601943\n",
            "125 0.0006755775539204478\n",
            "126 0.0006658848724327981\n",
            "127 0.0006563262431882322\n",
            "128 0.0006468832725659013\n",
            "129 0.0006375954253599048\n",
            "130 0.0006284293485805392\n",
            "131 0.0006193947629071772\n",
            "132 0.0006104722851887345\n",
            "133 0.0006017122650519013\n",
            "134 0.0005930728511884809\n",
            "135 0.0005845366977155209\n",
            "136 0.0005761412903666496\n",
            "137 0.0005678640445694327\n",
            "138 0.0005596935516223311\n",
            "139 0.0005516586825251579\n",
            "140 0.0005437259678728878\n",
            "141 0.0005359234055504203\n",
            "142 0.0005282106576487422\n",
            "143 0.0005206248024478555\n",
            "144 0.0005131326615810394\n",
            "145 0.0005057607777416706\n",
            "146 0.0004985020495951176\n",
            "147 0.0004913253942504525\n",
            "148 0.00048428389709442854\n",
            "149 0.0004773148684762418\n",
            "150 0.00047045311657711864\n",
            "151 0.0004636923549696803\n",
            "152 0.000457027112133801\n",
            "153 0.00045046384911984205\n",
            "154 0.00044398297904990613\n",
            "155 0.0004376000724732876\n",
            "156 0.0004313087265472859\n",
            "157 0.0004251150821801275\n",
            "158 0.0004190037143416703\n",
            "159 0.00041299033910036087\n",
            "160 0.00040706031722947955\n",
            "161 0.00040120058110915124\n",
            "162 0.0003954480343963951\n",
            "163 0.00038975829374976456\n",
            "164 0.00038415536982938647\n",
            "165 0.0003786269808188081\n",
            "166 0.00037319085095077753\n",
            "167 0.00036782523966394365\n",
            "168 0.0003625322424340993\n",
            "169 0.0003573292924556881\n",
            "170 0.00035219392157159746\n",
            "171 0.00034713640343397856\n",
            "172 0.00034213479375466704\n",
            "173 0.0003372260252945125\n",
            "174 0.0003323803539387882\n",
            "175 0.0003275970811955631\n",
            "176 0.00032289233058691025\n",
            "177 0.0003182501532137394\n",
            "178 0.000313683325657621\n",
            "179 0.0003091775579378009\n",
            "180 0.0003047359059564769\n",
            "181 0.00030034553492441773\n",
            "182 0.00029603103757835925\n",
            "183 0.00029177454416640103\n",
            "184 0.0002875838545151055\n",
            "185 0.0002834519254975021\n",
            "186 0.0002793770981952548\n",
            "187 0.0002753621665760875\n",
            "188 0.0002714011643547565\n",
            "189 0.0002675116411410272\n",
            "190 0.0002636639401316643\n",
            "191 0.00025986984837800264\n",
            "192 0.0002561437140684575\n",
            "193 0.0002524579467717558\n",
            "194 0.00024882820434868336\n",
            "195 0.0002452538756188005\n",
            "196 0.00024172892153728753\n",
            "197 0.0002382550446782261\n",
            "198 0.0002348294947296381\n",
            "199 0.00023146411695051938\n",
            "200 0.0002281287597725168\n",
            "201 0.00022484811779577285\n",
            "202 0.0002216189168393612\n",
            "203 0.00021843414288014174\n",
            "204 0.0002152967354049906\n",
            "205 0.0002121998113580048\n",
            "206 0.00020914759079460055\n",
            "207 0.000206148368306458\n",
            "208 0.00020318283350206912\n",
            "209 0.00020026387937832624\n",
            "210 0.00019738033006433398\n",
            "211 0.00019454893481452018\n",
            "212 0.00019175327906850725\n",
            "213 0.00018899944552686065\n",
            "214 0.00018627941608428955\n",
            "215 0.0001836094306781888\n",
            "216 0.0001809634268283844\n",
            "217 0.00017836378538049757\n",
            "218 0.0001757991558406502\n",
            "219 0.00017327605746686459\n",
            "220 0.00017078069504350424\n",
            "221 0.00016833358677104115\n",
            "222 0.00016590976156294346\n",
            "223 0.0001635284279473126\n",
            "224 0.0001611713378224522\n",
            "225 0.00015886174514889717\n",
            "226 0.0001565815764479339\n",
            "227 0.00015432221698574722\n",
            "228 0.0001521090161986649\n",
            "229 0.00014992497744970024\n",
            "230 0.0001477691112086177\n",
            "231 0.00014564675802830607\n",
            "232 0.0001435540325473994\n",
            "233 0.00014148718037176877\n",
            "234 0.00013945485989097506\n",
            "235 0.00013745464093517512\n",
            "236 0.0001354719279333949\n",
            "237 0.0001335224078502506\n",
            "238 0.0001316113630309701\n",
            "239 0.00012971411342732608\n",
            "240 0.00012785461149178445\n",
            "241 0.0001260111603187397\n",
            "242 0.0001242008584085852\n",
            "243 0.000122415556688793\n",
            "244 0.00012065721966791898\n",
            "245 0.00011892461043316871\n",
            "246 0.00011721524060703814\n",
            "247 0.00011553104559425265\n",
            "248 0.00011386680125724524\n",
            "249 0.00011223094043089077\n",
            "250 0.00011062435078201815\n",
            "251 0.0001090299483621493\n",
            "252 0.00010746339830802754\n",
            "253 0.00010591925820335746\n",
            "254 0.0001043984666466713\n",
            "255 0.00010289869533153251\n",
            "256 0.00010142091196030378\n",
            "257 9.996252629207447e-05\n",
            "258 9.852567018242553e-05\n",
            "259 9.710895392345265e-05\n",
            "260 9.571218106430024e-05\n",
            "261 9.433962986804545e-05\n",
            "262 9.298764780396596e-05\n",
            "263 9.164960647467524e-05\n",
            "264 9.032512753037736e-05\n",
            "265 8.903387060854584e-05\n",
            "266 8.775055903242901e-05\n",
            "267 8.649358642287552e-05\n",
            "268 8.524542499799281e-05\n",
            "269 8.402390812989324e-05\n",
            "270 8.281598275061697e-05\n",
            "271 8.162415906554088e-05\n",
            "272 8.045112917898223e-05\n",
            "273 7.92956561781466e-05\n",
            "274 7.815728895366192e-05\n",
            "275 7.703734445385635e-05\n",
            "276 7.592346810270101e-05\n",
            "277 7.483529770979658e-05\n",
            "278 7.375673158094287e-05\n",
            "279 7.270086643984541e-05\n",
            "280 7.165556598920375e-05\n",
            "281 7.062443910399452e-05\n",
            "282 6.961121107451618e-05\n",
            "283 6.861204019514844e-05\n",
            "284 6.762583507224917e-05\n",
            "285 6.665033288300037e-05\n",
            "286 6.569676043000072e-05\n",
            "287 6.474865949712694e-05\n",
            "288 6.382207357091829e-05\n",
            "289 6.290194141911343e-05\n",
            "290 6.199900963110849e-05\n",
            "291 6.110602407716215e-05\n",
            "292 6.0228543588891625e-05\n",
            "293 5.936635716352612e-05\n",
            "294 5.851061723660678e-05\n",
            "295 5.7670800742926076e-05\n",
            "296 5.6845819926820695e-05\n",
            "297 5.602439341600984e-05\n",
            "298 5.521953426068649e-05\n",
            "299 5.442714609671384e-05\n",
            "300 5.364090975490399e-05\n",
            "301 5.2870531362714246e-05\n",
            "302 5.211433017393574e-05\n",
            "303 5.136525578564033e-05\n",
            "304 5.0624490540940315e-05\n",
            "305 4.989732406102121e-05\n",
            "306 4.9182952352566645e-05\n",
            "307 4.847475793212652e-05\n",
            "308 4.777832145919092e-05\n",
            "309 4.70949053124059e-05\n",
            "310 4.6415421820711344e-05\n",
            "311 4.5748740376438946e-05\n",
            "312 4.509098653215915e-05\n",
            "313 4.44443867309019e-05\n",
            "314 4.3802850996144116e-05\n",
            "315 4.317780985729769e-05\n",
            "316 4.255272142472677e-05\n",
            "317 4.19425341533497e-05\n",
            "318 4.1340677853440866e-05\n",
            "319 4.0747076127445325e-05\n",
            "320 4.016033926745877e-05\n",
            "321 3.958043089369312e-05\n",
            "322 3.901362288161181e-05\n",
            "323 3.84541526727844e-05\n",
            "324 3.789837137446739e-05\n",
            "325 3.7356046959757805e-05\n",
            "326 3.681886664708145e-05\n",
            "327 3.6288554838392884e-05\n",
            "328 3.5770779504673555e-05\n",
            "329 3.5255172406323254e-05\n",
            "330 3.4747594327200204e-05\n",
            "331 3.4250158932991326e-05\n",
            "332 3.376002860022709e-05\n",
            "333 3.327192825963721e-05\n",
            "334 3.279503289377317e-05\n",
            "335 3.232389281038195e-05\n",
            "336 3.185845707776025e-05\n",
            "337 3.140080298180692e-05\n",
            "338 3.09482347802259e-05\n",
            "339 3.0504254027619027e-05\n",
            "340 3.006777842529118e-05\n",
            "341 2.96330290439073e-05\n",
            "342 2.9208669729996473e-05\n",
            "343 2.8787684641429223e-05\n",
            "344 2.8375328838592395e-05\n",
            "345 2.7968562790192664e-05\n",
            "346 2.7565343771129847e-05\n",
            "347 2.7169597160536796e-05\n",
            "348 2.6778368919622153e-05\n",
            "349 2.6394007363705896e-05\n",
            "350 2.6013465685537085e-05\n",
            "351 2.564069291111082e-05\n",
            "352 2.5271196136600338e-05\n",
            "353 2.4908151317504235e-05\n",
            "354 2.4548884539399296e-05\n",
            "355 2.419594420643989e-05\n",
            "356 2.384670005994849e-05\n",
            "357 2.350647082494106e-05\n",
            "358 2.3169248379417695e-05\n",
            "359 2.2835994968772866e-05\n",
            "360 2.250611942145042e-05\n",
            "361 2.218233930761926e-05\n",
            "362 2.1865253074793145e-05\n",
            "363 2.1552881662501022e-05\n",
            "364 2.1241150534478948e-05\n",
            "365 2.093701004923787e-05\n",
            "366 2.0633877284126356e-05\n",
            "367 2.0338982722023502e-05\n",
            "368 2.004790076171048e-05\n",
            "369 1.975788654817734e-05\n",
            "370 1.9474986402201466e-05\n",
            "371 1.9193494154023938e-05\n",
            "372 1.891859938041307e-05\n",
            "373 1.8648195691639557e-05\n",
            "374 1.8379121684120037e-05\n",
            "375 1.8113980331690982e-05\n",
            "376 1.7853710232884623e-05\n",
            "377 1.759813130775001e-05\n",
            "378 1.734451507218182e-05\n",
            "379 1.7097423551604152e-05\n",
            "380 1.685115603322629e-05\n",
            "381 1.6606674762442708e-05\n",
            "382 1.636762681300752e-05\n",
            "383 1.613509448361583e-05\n",
            "384 1.5900408470770344e-05\n",
            "385 1.5672614608774893e-05\n",
            "386 1.5448747944901697e-05\n",
            "387 1.5225133211060893e-05\n",
            "388 1.5005400200607255e-05\n",
            "389 1.479128604842117e-05\n",
            "390 1.4577382899005897e-05\n",
            "391 1.4369001291925088e-05\n",
            "392 1.4164747881295625e-05\n",
            "393 1.3958919225842692e-05\n",
            "394 1.3757397937297355e-05\n",
            "395 1.3561512787418906e-05\n",
            "396 1.3366186067287344e-05\n",
            "397 1.3173224942875095e-05\n",
            "398 1.29846011986956e-05\n",
            "399 1.2798271200153977e-05\n",
            "400 1.2613700164365582e-05\n",
            "401 1.24329317259253e-05\n",
            "402 1.2255504771019332e-05\n",
            "403 1.2077129213139415e-05\n",
            "404 1.1903270205948502e-05\n",
            "405 1.1733060091501102e-05\n",
            "406 1.156417783931829e-05\n",
            "407 1.1396913578209933e-05\n",
            "408 1.1233298209845088e-05\n",
            "409 1.10713535832474e-05\n",
            "410 1.0913653568422887e-05\n",
            "411 1.0757564268715214e-05\n",
            "412 1.0602317161101382e-05\n",
            "413 1.0450076842971612e-05\n",
            "414 1.030052408168558e-05\n",
            "415 1.0153251423616894e-05\n",
            "416 1.0005664080381393e-05\n",
            "417 9.861800208454952e-06\n",
            "418 9.719707122712862e-06\n",
            "419 9.580620826454833e-06\n",
            "420 9.442896043765359e-06\n",
            "421 9.307674190495163e-06\n",
            "422 9.173518265015446e-06\n",
            "423 9.042519195645582e-06\n",
            "424 8.911509212339297e-06\n",
            "425 8.782922122918535e-06\n",
            "426 8.656037607579492e-06\n",
            "427 8.531859748472925e-06\n",
            "428 8.409082511207089e-06\n",
            "429 8.289204743050504e-06\n",
            "430 8.169606189767364e-06\n",
            "431 8.053928468143567e-06\n",
            "432 7.935967005323619e-06\n",
            "433 7.82253027864499e-06\n",
            "434 7.711283615208231e-06\n",
            "435 7.600592653034255e-06\n",
            "436 7.490782991226297e-06\n",
            "437 7.382244803011417e-06\n",
            "438 7.276146789081395e-06\n",
            "439 7.17182774678804e-06\n",
            "440 7.06903483660426e-06\n",
            "441 6.967292392801028e-06\n",
            "442 6.867354386486113e-06\n",
            "443 6.768590537831187e-06\n",
            "444 6.670617949566804e-06\n",
            "445 6.575519364560023e-06\n",
            "446 6.480661795649212e-06\n",
            "447 6.388257133949082e-06\n",
            "448 6.296221727097873e-06\n",
            "449 6.205653335200623e-06\n",
            "450 6.115095857239794e-06\n",
            "451 6.028411007719114e-06\n",
            "452 5.94107223150786e-06\n",
            "453 5.85612633585697e-06\n",
            "454 5.7722827477846295e-06\n",
            "455 5.68849100091029e-06\n",
            "456 5.60682337891194e-06\n",
            "457 5.526909262698609e-06\n",
            "458 5.447162038763054e-06\n",
            "459 5.36974494025344e-06\n",
            "460 5.291612978908233e-06\n",
            "461 5.214783413975965e-06\n",
            "462 5.141215297044255e-06\n",
            "463 5.0661446948652156e-06\n",
            "464 4.994612936570775e-06\n",
            "465 4.922492280456936e-06\n",
            "466 4.851217454415746e-06\n",
            "467 4.782305950357113e-06\n",
            "468 4.713066573458491e-06\n",
            "469 4.64539743916248e-06\n",
            "470 4.578840162139386e-06\n",
            "471 4.512391569733154e-06\n",
            "472 4.448022991709877e-06\n",
            "473 4.385151896713069e-06\n",
            "474 4.321218966651941e-06\n",
            "475 4.259073648427147e-06\n",
            "476 4.197021553409286e-06\n",
            "477 4.138083568250295e-06\n",
            "478 4.078507117810659e-06\n",
            "479 4.0190143408835866e-06\n",
            "480 3.9614624256500974e-06\n",
            "481 3.90449622500455e-06\n",
            "482 3.8489129110530484e-06\n",
            "483 3.7936156331852544e-06\n",
            "484 3.7390561828942737e-06\n",
            "485 3.6843891848548083e-06\n",
            "486 3.6326164263300598e-06\n",
            "487 3.5807161111733876e-06\n",
            "488 3.5277698771096766e-06\n",
            "489 3.4785766729328316e-06\n",
            "490 3.42682551490725e-06\n",
            "491 3.378343762960867e-06\n",
            "492 3.330951130919857e-06\n",
            "493 3.2827349514263915e-06\n",
            "494 3.2344005376216955e-06\n",
            "495 3.188760047123651e-06\n",
            "496 3.142618197671254e-06\n",
            "497 3.0976830203144345e-06\n",
            "498 3.0527676244673785e-06\n",
            "499 3.008934527315432e-06\n",
            "predict (after training) 4 tensor(8.0020)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lecture 6\n",
        "\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model,self).__init__()\n",
        "        self.linear = torch.nn.Linear(1,1)\n",
        "    def forward(self,x):\n",
        "        y_pred = F.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)"
      ],
      "metadata": {
        "id": "Vtau_RCuvmNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c20c76-8e09-4c2b-cbc2-a35c9710bc9d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lecture 7\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model,self).__init__()\n",
        "        self.l1 = torch.nn.Linear(8,6)\n",
        "        self.l2 = torch.nn.Linear(6,4)\n",
        "        self.l3 = torch.nn.Linear(4,1)\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))"
      ],
      "metadata": {
        "id": "l9QxwuZ1rFxF"
      },
      "execution_count": 83,
      "outputs": []
    }
  ]
}